{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d4077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset...\n",
      "Creating date features...\n",
      "Adding brand/city frequency encoding...\n",
      "Aggregating to daily demand...\n",
      "Category and country rolling windows...\n",
      "Creating lag features...\n",
      "Creating rolling-window features...\n",
      "Applying time-based split...\n",
      "Applying TOP-N product grouping...\n",
      "Applying frequency encoding...\n",
      "Applying target encoding...\n",
      "Label encoding columns...\n",
      "Dropping NA rows...\n",
      "Saving final engineered datasets...\n",
      "\n",
      "Feature Engineering v2 Complete ✓\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# FEATURE ENGINEERING PIPELINE (VERSION 2 - IMPROVED)\n",
    "# Massive upgrade: richer seasonal features, extra lags,\n",
    "# rolling windows, brand/city encodings, interactions, etc.\n",
    "# ===========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df = pd.read_csv(\"../data/cleaned_retail_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# ===========================================================\n",
    "# 1. DATE FEATURES\n",
    "# ===========================================================\n",
    "print(\"Creating date features...\")\n",
    "\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['weekday'] = df['Date'].dt.weekday\n",
    "df['weekofyear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "df['dayofyear'] = df['Date'].dt.dayofyear\n",
    "\n",
    "df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "\n",
    "# cyclic (sin/cos) encoding for seasonality\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# ===========================================================\n",
    "# 2. ADD BRAND & CITY FREQUENCY ENCODING\n",
    "# ===========================================================\n",
    "print(\"Adding brand/city frequency encoding...\")\n",
    "\n",
    "df['Product_Brand_freq'] = df['Product_Brand'].map(df['Product_Brand'].value_counts(normalize=True))\n",
    "df['City_freq'] = df['City'].map(df['City'].value_counts(normalize=True))\n",
    "\n",
    "# ===========================================================\n",
    "# 3. DAILY AGGREGATION (Project Objective)\n",
    "# ===========================================================\n",
    "print(\"Aggregating to daily demand...\")\n",
    "\n",
    "daily = (\n",
    "    df.groupby(['Date','Product_Category','Country'])\n",
    "      .agg({\n",
    "           'Total_Purchases':'sum',\n",
    "           'Amount':'mean',\n",
    "           'Total_Amount':'mean',\n",
    "           'Product_Brand_freq':'mean',\n",
    "           'City_freq':'mean'\n",
    "      })\n",
    "      .reset_index()\n",
    "      .sort_values(['Product_Category','Country','Date'])\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 4. ADD CATEGORY & COUNTRY ROLLING FEATURES\n",
    "# ===========================================================\n",
    "print(\"Category and country rolling windows...\")\n",
    "\n",
    "daily['cat_roll_30'] = daily.groupby('Product_Category')['Total_Purchases'].transform(lambda x: x.shift(1).rolling(30).mean())\n",
    "daily['country_roll_30'] = daily.groupby('Country')['Total_Purchases'].transform(lambda x: x.shift(1).rolling(30).mean())\n",
    "\n",
    "# ===========================================================\n",
    "# 5. LAGS (1,2,3,7,14,21)\n",
    "# ===========================================================\n",
    "print(\"Creating lag features...\")\n",
    "\n",
    "grp = daily.groupby(['Product_Category','Country'])['Total_Purchases']\n",
    "\n",
    "daily['lag_1']  = grp.shift(1)\n",
    "daily['lag_2']  = grp.shift(2)\n",
    "daily['lag_3']  = grp.shift(3)\n",
    "\n",
    "daily['lag_7']  = grp.shift(7)\n",
    "daily['lag_14'] = grp.shift(14)\n",
    "daily['lag_21'] = grp.shift(21)\n",
    "\n",
    "# ===========================================================\n",
    "# 6. ROLLING WINDOWS (7,14,30)\n",
    "# ===========================================================\n",
    "print(\"Creating rolling-window features...\")\n",
    "\n",
    "daily['roll_mean_7']  = grp.shift(1).rolling(7).mean()\n",
    "daily['roll_std_7']   = grp.shift(1).rolling(7).std()\n",
    "\n",
    "daily['roll_mean_14'] = grp.shift(1).rolling(14).mean()\n",
    "daily['roll_std_14']  = grp.shift(1).rolling(14).std()\n",
    "\n",
    "daily['roll_mean_30'] = grp.shift(1).rolling(30).mean()\n",
    "daily['roll_std_30']  = grp.shift(1).rolling(30).std()\n",
    "\n",
    "# ===========================================================\n",
    "# 7. TRAIN / TEST SPLIT (TIME-BASED)\n",
    "# ===========================================================\n",
    "print(\"Applying time-based split...\")\n",
    "\n",
    "split_date = daily['Date'].max() - pd.Timedelta(days=60)\n",
    "train_daily = daily[daily['Date'] <= split_date].copy()\n",
    "test_daily  = daily[daily['Date']  > split_date].copy()\n",
    "\n",
    "# ===========================================================\n",
    "# 8. TOP-N PRODUCT GROUPING (TRAIN ONLY)\n",
    "# ===========================================================\n",
    "print(\"Applying TOP-N product grouping...\")\n",
    "\n",
    "TOP_N = 50\n",
    "top_products = df['products'].value_counts().head(TOP_N).index.tolist()\n",
    "\n",
    "df['products_grouped'] = df['products'].apply(lambda x: x if x in top_products else 'OTHER')\n",
    "\n",
    "train_daily = train_daily.merge(\n",
    "    df[['Date','Product_Category','Country','products_grouped']],\n",
    "    on=['Date','Product_Category','Country'], how='left'\n",
    ")\n",
    "test_daily = test_daily.merge(\n",
    "    df[['Date','Product_Category','Country','products_grouped']],\n",
    "    on=['Date','Product_Category','Country'], how='left'\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 9. FREQUENCY ENCODING (TRAIN ONLY)\n",
    "# ===========================================================\n",
    "print(\"Applying frequency encoding...\")\n",
    "\n",
    "freq = train_daily['products_grouped'].value_counts(normalize=True)\n",
    "train_daily['product_freq'] = train_daily['products_grouped'].map(freq)\n",
    "test_daily['product_freq']  = test_daily['products_grouped'].map(freq).fillna(0)\n",
    "\n",
    "# ===========================================================\n",
    "# 10. TARGET ENCODING (TRAIN ONLY)\n",
    "# ===========================================================\n",
    "print(\"Applying target encoding...\")\n",
    "\n",
    "global_mean = train_daily['Total_Purchases'].mean()\n",
    "target_enc = train_daily.groupby('products_grouped')['Total_Purchases'].mean()\n",
    "\n",
    "train_daily['product_target_enc'] = train_daily['products_grouped'].map(target_enc)\n",
    "test_daily['product_target_enc']  = test_daily['products_grouped'].map(target_enc).fillna(global_mean)\n",
    "\n",
    "# ===========================================================\n",
    "# 11. LABEL ENCODING (Category + Country + Product Group)\n",
    "# ===========================================================\n",
    "print(\"Label encoding columns...\")\n",
    "\n",
    "LE_cols = ['Product_Category','Country','products_grouped']\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in LE_cols:\n",
    "    train_daily[col] = le.fit_transform(train_daily[col].astype(str))\n",
    "    test_daily[col]  = le.transform(test_daily[col].astype(str))\n",
    "\n",
    "# ===========================================================\n",
    "# 12. DROP NA after lags\n",
    "# ===========================================================\n",
    "print(\"Dropping NA rows...\")\n",
    "\n",
    "train_daily = train_daily.dropna().reset_index(drop=True)\n",
    "test_daily  = test_daily.dropna().reset_index(drop=True)\n",
    "\n",
    "# ===========================================================\n",
    "# 13. SAVE FINAL FILES\n",
    "# ===========================================================\n",
    "print(\"Saving final engineered datasets...\")\n",
    "\n",
    "train_daily.to_csv(\"../data/final_train_v2.csv\", index=False)\n",
    "test_daily.to_csv(\"../data/final_test_v2.csv\", index=False)\n",
    "\n",
    "print(\"\\nFeature Engineering v2 Complete ✓\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
