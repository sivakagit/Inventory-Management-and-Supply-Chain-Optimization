{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f28a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset...\n",
      "Creating transaction-level date features...\n",
      "Applying time-based transaction split (train/test)...\n",
      "transactions: total=301290, train=251542, test=49748\n",
      "Computing train-only frequency encodings for Product_Brand and City...\n",
      "Creating TOP-N product grouping from train transactions...\n",
      "Aggregating to daily demand (per Product_Category x Country x Date)...\n",
      "Merging product grouping into daily (deduplicated)...\n",
      "Creating lags and rolling windows on aggregated data...\n",
      "Category and country-level rolling features...\n",
      "Splitting aggregated data into train/test (daily)...\n",
      "daily rows: total=64245, train=53705, test=10540)\n",
      "Applying frequency encoding for products_grouped (train-only)...\n",
      "Applying smoothed target encoding for products_grouped (train-only)...\n",
      "Label encoding category/country/products_grouped with independent encoders...\n",
      "Dropping rows with NA introduced by lag/rolling features in TRAIN only (keep test as-is to inspect)...\n",
      "Dropped 735 rows from train due to NA (lags/rolls).\n",
      "Saving final datasets and encoders...\n",
      "Feature Engineering v3 Complete ✓\n",
      "Saved: ..\\data\\final_train_v3.csv, ..\\data\\final_test_v3.csv, ..\\data\\encoders_v3.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Engineering Pipeline — Version 3 (Leakage-free, robust, train-only encodings)\n",
    "Save as: feature_engineering_v3.py\n",
    "\n",
    "Key improvements over v2:\n",
    " - compute train/test split on transaction-level BEFORE any train-only encodings\n",
    " - frequency & brand encodings computed only on train and mapped to both sets\n",
    " - deduplicate before merging transactional-level product grouping into daily\n",
    " - per-column LabelEncoder instances (saved)\n",
    " - safe target encoding (smoothed) computed on train only\n",
    " - robust rolling / lag computation using group transforms\n",
    " - checks for missing columns and informative errors\n",
    " - saves engineered datasets and pickled encoders\n",
    "\n",
    "Usage: run in same environment as your data; adjust paths and TOP_N as needed.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ------------------------\n",
    "# Config\n",
    "# ------------------------\n",
    "RAW_PATH = Path(\"../data/cleaned_retail_data.csv\")\n",
    "OUT_DIR = Path(\"../data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_OUT = OUT_DIR / \"final_train_v3.csv\"\n",
    "TEST_OUT = OUT_DIR / \"final_test_v3.csv\"\n",
    "ENCODERS_OUT = OUT_DIR / \"encoders_v3.pkl\"\n",
    "PICKLE_PROTOCOL = 4\n",
    "TOP_N = 50  # top N products for grouping\n",
    "ROLL_WINDOWS = [7, 14, 30]\n",
    "LAGS = [1, 2, 3, 7, 14, 21]\n",
    "\n",
    "# ------------------------\n",
    "# Helpers\n",
    "# ------------------------\n",
    "\n",
    "def ensure_columns(df, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns in input: {missing}\")\n",
    "\n",
    "\n",
    "def smoothed_target_encoding(series, target, alpha=10):\n",
    "    \"\"\"\n",
    "    Smoothed target encoding (train-only). Returns mapping dict.\n",
    "    alpha: higher -> stronger smoothing toward global mean.\n",
    "    \"\"\"\n",
    "    stats = target.groupby(series).agg(['mean', 'count'])\n",
    "    global_mean = target.mean()\n",
    "    # smoothed = (count * mean + alpha * global_mean) / (count + alpha)\n",
    "    smoothed = ((stats['count'] * stats['mean']) + (alpha * global_mean)) / (stats['count'] + alpha)\n",
    "    return smoothed.to_dict(), global_mean\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Load and validate\n",
    "# ------------------------\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df = pd.read_csv(RAW_PATH, parse_dates=[\"Date\"])  # will raise if file missing\n",
    "\n",
    "# required transactional columns\n",
    "required_cols = [\"Date\", \"Product_Category\", \"Country\", \"Total_Purchases\", \"Product_Brand\", \"City\"]\n",
    "ensure_columns(df, required_cols)\n",
    "\n",
    "# decide product column name\n",
    "product_col = None\n",
    "for cand in [\"products\", \"Product_Name\", \"Product\", \"SKU\"]:\n",
    "    if cand in df.columns:\n",
    "        product_col = cand\n",
    "        break\n",
    "\n",
    "if product_col is None:\n",
    "    # we don't crash — create a placeholder by category if product-level is missing\n",
    "    product_col = 'products'\n",
    "    df[product_col] = df['Product_Category'].astype(str) + \"::UNKNOWN_PRODUCT\"\n",
    "    print(f\"Warning: no explicit product column found. Created placeholder column '{product_col}'.\")\n",
    "\n",
    "# ------------------------\n",
    "# 1) Basic date features on transactions\n",
    "# ------------------------\n",
    "print(\"Creating transaction-level date features...\")\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['weekday'] = df['Date'].dt.weekday\n",
    "\n",
    "# cyclic encodings\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# ------------------------\n",
    "# 2) Time-based split ON TRANSACTION LEVEL (avoid leakage)\n",
    "# ------------------------\n",
    "print(\"Applying time-based transaction split (train/test)...\")\n",
    "cut_date = df['Date'].max() - pd.Timedelta(days=60)\n",
    "trans_train = df[df['Date'] <= cut_date].copy()\n",
    "trans_test = df[df['Date'] >  cut_date].copy()\n",
    "\n",
    "print(f\"transactions: total={len(df)}, train={len(trans_train)}, test={len(trans_test)}\")\n",
    "\n",
    "# ------------------------\n",
    "# 3) Frequency encodings computed on train transactions and mapped everywhere\n",
    "# ------------------------\n",
    "print(\"Computing train-only frequency encodings for Product_Brand and City...\")\n",
    "brand_freq = trans_train['Product_Brand'].value_counts(normalize=True)\n",
    "city_freq  = trans_train['City'].value_counts(normalize=True)\n",
    "\n",
    "# map back to full df (we map using df index to keep alignment), fill missing with 0\n",
    "for dset in (df, trans_train, trans_test):\n",
    "    dset['Product_Brand_freq'] = dset['Product_Brand'].map(brand_freq).fillna(0.0)\n",
    "    dset['City_freq'] = dset['City'].map(city_freq).fillna(0.0)\n",
    "\n",
    "# ------------------------\n",
    "# 4) Create products_grouped using TOP_N from train transactions\n",
    "# ------------------------\n",
    "print(\"Creating TOP-N product grouping from train transactions...\")\n",
    "TOP_products = trans_train[product_col].value_counts().head(TOP_N).index.tolist()\n",
    "\n",
    "def group_product(x):\n",
    "    return x if x in TOP_products else 'OTHER'\n",
    "\n",
    "for dset in (df, trans_train, trans_test):\n",
    "    dset['products_grouped'] = dset[product_col].apply(group_product)\n",
    "\n",
    "# deduplicate mapping table for later merge\n",
    "prod_mapping = df[['Date', 'Product_Category', 'Country', 'products_grouped']].drop_duplicates()\n",
    "\n",
    "# ------------------------\n",
    "# 5) Aggregate to daily level (on the full set) — use transaction-level freq enc columns\n",
    "# ------------------------\n",
    "print(\"Aggregating to daily demand (per Product_Category x Country x Date)...\")\n",
    "\n",
    "agg_cols = {\n",
    "    'Total_Purchases': 'sum',\n",
    "    # keep a few transaction-level aggregations as features\n",
    "    'Product_Brand_freq': 'mean',\n",
    "    'City_freq': 'mean'\n",
    "}\n",
    "\n",
    "# if 'Amount' exists, include it; same for 'Total_Amount'\n",
    "for optional in ['Amount', 'Total_Amount']:\n",
    "    if optional in df.columns:\n",
    "        agg_cols[optional] = 'mean'\n",
    "\n",
    "\n",
    "daily = (\n",
    "    df.groupby(['Date', 'Product_Category', 'Country'])\n",
    "      .agg(agg_cols)\n",
    "      .reset_index()\n",
    "      .sort_values(['Product_Category', 'Country', 'Date'])\n",
    ")\n",
    "\n",
    "# ------------------------\n",
    "# 6) Merge deduped product grouping into daily safely\n",
    "# ------------------------\n",
    "print(\"Merging product grouping into daily (deduplicated)...\")\n",
    "daily = daily.merge(prod_mapping, on=['Date','Product_Category','Country'], how='left')\n",
    "\n",
    "# if some daily rows miss grouping, fill as 'OTHER'\n",
    "daily['products_grouped'] = daily['products_grouped'].fillna('OTHER')\n",
    "\n",
    "# ------------------------\n",
    "# 7) Now compute rolling and lag features grouped by Product_Category + Country\n",
    "# ------------------------\n",
    "print(\"Creating lags and rolling windows on aggregated data...\")\n",
    "\n",
    "daily = daily.sort_values(['Product_Category','Country','Date']).reset_index(drop=True)\n",
    "\n",
    "group_cols = ['Product_Category','Country']\n",
    "\n",
    "def group_transform(df, colname, fn):\n",
    "    return df.groupby(group_cols)[colname].transform(fn)\n",
    "\n",
    "# LAGS\n",
    "for lag in LAGS:\n",
    "    daily[f'lag_{lag}'] = daily.groupby(group_cols)['Total_Purchases'].shift(lag)\n",
    "\n",
    "# Rolling windows — use shift(1) to avoid including current day\n",
    "for w in ROLL_WINDOWS:\n",
    "    daily[f'roll_mean_{w}'] = daily.groupby(group_cols)['Total_Purchases'].transform(lambda x: x.shift(1).rolling(window=w, min_periods=1).mean())\n",
    "    daily[f'roll_std_{w}']  = daily.groupby(group_cols)['Total_Purchases'].transform(lambda x: x.shift(1).rolling(window=w, min_periods=1).std())\n",
    "\n",
    "# Category and country level rolling (30-day) — separate features\n",
    "print(\"Category and country-level rolling features...\")\n",
    "daily['cat_roll_30'] = daily.groupby('Product_Category')['Total_Purchases'].transform(lambda x: x.shift(1).rolling(30, min_periods=1).mean())\n",
    "daily['country_roll_30'] = daily.groupby('Country')['Total_Purchases'].transform(lambda x: x.shift(1).rolling(30, min_periods=1).mean())\n",
    "\n",
    "# ------------------------\n",
    "# 8) Time-based split on aggregated DAILY data (same cut_date)\n",
    "# ------------------------\n",
    "print(\"Splitting aggregated data into train/test (daily)...\")\n",
    "train_daily = daily[daily['Date'] <= cut_date].copy()\n",
    "test_daily  = daily[daily['Date'] >  cut_date].copy()\n",
    "\n",
    "print(f\"daily rows: total={len(daily)}, train={len(train_daily)}, test={len(test_daily)})\")\n",
    "\n",
    "# ------------------------\n",
    "# 9) Frequency encoding of products_grouped (train-only)\n",
    "# ------------------------\n",
    "print(\"Applying frequency encoding for products_grouped (train-only)...\")\n",
    "freq_map = train_daily['products_grouped'].value_counts(normalize=True).to_dict()\n",
    "train_daily['product_freq'] = train_daily['products_grouped'].map(freq_map)\n",
    "test_daily['product_freq']  = test_daily['products_grouped'].map(freq_map).fillna(0.0)\n",
    "\n",
    "# ------------------------\n",
    "# 10) Smoothed target encoding (train-only)\n",
    "# ------------------------\n",
    "print(\"Applying smoothed target encoding for products_grouped (train-only)...\")\n",
    "te_map, global_mean = smoothed_target_encoding(train_daily['products_grouped'], train_daily['Total_Purchases'], alpha=20)\n",
    "train_daily['product_target_enc'] = train_daily['products_grouped'].map(te_map)\n",
    "test_daily['product_target_enc']  = test_daily['products_grouped'].map(te_map).fillna(global_mean)\n",
    "\n",
    "# ------------------------\n",
    "# 11) Label Encoding (per-column encoders saved)\n",
    "# ------------------------\n",
    "print(\"Label encoding category/country/products_grouped with independent encoders...\")\n",
    "LE_cols = ['Product_Category','Country','products_grouped']\n",
    "\n",
    "encoders = {}\n",
    "for col in LE_cols:\n",
    "    le = LabelEncoder()\n",
    "    # fit on train union test categories seen in train+test to avoid unseen error in transform later\n",
    "    combined = pd.concat([train_daily[col].astype(str), test_daily[col].astype(str)]).unique()\n",
    "    le.fit(combined)\n",
    "    train_daily[col + '_le'] = le.transform(train_daily[col].astype(str))\n",
    "    test_daily[col + '_le']  = le.transform(test_daily[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# ------------------------\n",
    "# 12) Drop rows with NA that are unavoidable (e.g., first N days for lags)\n",
    "# ------------------------\n",
    "print(\"Dropping rows with NA introduced by lag/rolling features in TRAIN only (keep test as-is to inspect)...\")\n",
    "pre_drop_train = len(train_daily)\n",
    "train_daily = train_daily.dropna().reset_index(drop=True)\n",
    "post_drop_train = len(train_daily)\n",
    "print(f\"Dropped {pre_drop_train - post_drop_train} rows from train due to NA (lags/rolls).\")\n",
    "\n",
    "# For test, it's often useful to keep NA (or impute). We'll drop rows where the target is NA (shouldn't be) but keep feature NA if you plan to impute later.\n",
    "# If you want to drop NA in test as well, uncomment below line.\n",
    "# test_daily = test_daily.dropna().reset_index(drop=True)\n",
    "\n",
    "# ------------------------\n",
    "# 13) Persist artifacts and final datasets\n",
    "# ------------------------\n",
    "print(\"Saving final datasets and encoders...\")\n",
    "train_daily.to_csv(TRAIN_OUT, index=False)\n",
    "test_daily.to_csv(TEST_OUT, index=False)\n",
    "with open(ENCODERS_OUT, 'wb') as f:\n",
    "    pickle.dump({'label_encoders': encoders, 'brand_freq': brand_freq.to_dict(), 'city_freq': city_freq.to_dict(), 'product_top_n': TOP_products, 'target_encoding_map': te_map, 'target_global_mean': global_mean}, f, protocol=PICKLE_PROTOCOL)\n",
    "\n",
    "print(\"Feature Engineering v3 Complete ✓\")\n",
    "print(f\"Saved: {TRAIN_OUT}, {TEST_OUT}, {ENCODERS_OUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
